{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6207dee-e5c2-4766-91e2-2e92839d9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2b3bb-0181-48c0-9ef6-9ebc5d817f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parse pandas df. Extract columns, change start/end, add position, delete first and last 256 bp\n",
    "# drop_nan_strategy = any - will delete columns with NaN in any column. This is for Zero Shot\n",
    "# drop_nan_strategy = both - delete only if Nan in both; is for Fine Tuning\n",
    "def prepare_df_for_PlantCad_zeroshot(inp_df, ref_col, alt_col, frame=512, drop_nan_strategy=\"any\"):\n",
    "    df = inp_df.copy()\n",
    "    df = df[[\"chr\", \"start\", \"end\", ref_col, alt_col]]\n",
    "    \n",
    "    # Удаление NaN по выбранной стратегии\n",
    "    if drop_nan_strategy == \"any\":\n",
    "        df = df.dropna(subset=[ref_col, alt_col], how=\"any\")\n",
    "    elif drop_nan_strategy == \"both\":\n",
    "        df = df[~(df[ref_col].isna() & df[alt_col].isna())]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid drop_nan_strategy. Choose 'any' or 'both'.\")\n",
    "    \n",
    "    # Применяем фильтр для удаления строк, где 'start' меньше (frame/2 - 1)\n",
    "    df = df[df[\"start\"] >= (frame // 2 - 1)]\n",
    "    \n",
    "    # Создаем DataFrame df_zero_shot_input_coordinates. Координаты pos начинаются с 1\n",
    "    df_zero_shot_input_coordinates = pd.DataFrame({\n",
    "        \"chr\": df[\"chr\"],\n",
    "        \"start\": df[\"start\"] - (frame // 2 - 1),\n",
    "        \"end\": df[\"start\"] + 1 + (frame // 2),\n",
    "        \"pos\": df[\"start\"] + 1,\n",
    "        \"ref\": df[ref_col],\n",
    "        \"alt\": df[alt_col]\n",
    "    })\n",
    "    \n",
    "    return df_zero_shot_input_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781252be-2cf5-45d8-aa2f-0dfac6fe1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seqs_to_df_input_coordinates(genome_fasta, df_zero_shot_input_coordinates):\n",
    "    # Read the genome FASTA file\n",
    "    genome = {record.id: record.seq for record in SeqIO.parse(genome_fasta, \"fasta\")}\n",
    "    \n",
    "    # Define a function to extract the sequence based on coordinates\n",
    "    def extract_sequence(row):\n",
    "        chrom = row['chr']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        # Check if chromosome exists in genome\n",
    "        if chrom in genome:\n",
    "            # Check if 'end' is greater than the sequence length\n",
    "            seq_length = len(genome[chrom])\n",
    "            if end > seq_length:\n",
    "                return None\n",
    "            # Extract the subsequence, convert to string and uppercase\n",
    "            subsequence = genome[chrom][start:end] \n",
    "            return str(subsequence).upper()\n",
    "        else:\n",
    "            return None \n",
    "\n",
    "    # Add progress bar to the DataFrame processing\n",
    "    tqdm.pandas(desc=\"Adding sequences\")\n",
    "    df_zero_shot_input_coordinates['sequences'] = df_zero_shot_input_coordinates.progress_apply(extract_sequence, axis=1)\n",
    "    \n",
    "    # Drop rows where 'sequence' is None (i.e., where 'end' was out of bounds)\n",
    "    df_zero_shot_input_coordinates = df_zero_shot_input_coordinates.dropna(subset=['sequences'])\n",
    "    \n",
    "    return df_zero_shot_input_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda88a16-21c3-4831-8a9b-de35c0336e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 255 to ref state\n",
    "def correct_sequence_ref(inp_df):\n",
    "    df = inp_df.copy()\n",
    "    \n",
    "    def modify_sequence(row):\n",
    "        # Заменяем символ на 255-й позиции в зависимости от label\n",
    "        char_to_replace = row[\"ref\"]\n",
    "        sequence = row[\"sequences\"]\n",
    "        modified_sequence = sequence[:255] + char_to_replace + sequence[256:]\n",
    "        return modified_sequence\n",
    "    # Применяем функцию с прогрессом к DataFrame\n",
    "    tqdm.pandas(desc=\"Correcting sequences\")\n",
    "    df[\"sequences\"] = df.progress_apply(modify_sequence, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae88500-5556-4e92-8333-9dfa3cf069a8",
   "metadata": {},
   "source": [
    "# Prepare files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc00d21-d480-461f-8607-3171de61d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bed files as input:\n",
    "# chr     start   end     ref     alt\n",
    "# Chr1A   119182  119183  A       C\n",
    "# Chr1A   119183  119184  C       A\n",
    "# Chr1A   119192  119193  C       T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c40de6-901d-4bac-adb7-9e0e10047120",
   "metadata": {},
   "outputs": [],
   "source": [
    "allele_dataset = \"/home/labs/alevy/petrzhu/Wheat/1k_project_liftover_v2.1/SNPs_lifted_final2_sorted_v1.bed\" #\"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/TAIR10_allele_dataset.txt.gz\"\n",
    "genome_fasta = \"/home/labs/alevy/petrzhu/Prog/Bitbucket_msa/iwgsc_refseqv2.1/iwgsc_refseqv2.1_assembly.fa\" #\"/home/labs/alevy/omerbar/backups/TAIR/A_thaliana.fa\"\n",
    "output_file = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/Wheat/ZH_1K_exomes.txt\" #\"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/TAIR10_ZH_neutral_vs_simulated.txt\"\n",
    "README = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/Wheat/REAMDE.txt\"\n",
    "ref_col = 'ref' \n",
    "alt_col = 'alt'\n",
    "\n",
    "print('loading dataset')\n",
    "df_allele_dataset = pd.read_csv(allele_dataset, sep=\"\\t\", header=0, compression='gzip', na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"])\n",
    "print('change coordinates')\n",
    "df_zero_shot_input_coordinates = prepare_df_for_PlantCad_zeroshot(df_allele_dataset, ref_col, alt_col)\n",
    "df_zero_shot_input = add_seqs_to_df_input_coordinates(genome_fasta, df_zero_shot_input_coordinates)\n",
    "df_zero_shot_input_corr = correct_sequence_ref(df_zero_shot_input)\n",
    "\n",
    "df_zero_shot_input_corr.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "\n",
    "readme_text = f\"{output_file}: ref_col = {ref_col}, alt_col = {alt_col}. VCF = /home/labs/alevy/petrzhu/Wheat/1k_project_liftover_v2.1/SNPs_lifted_final2_sorted_v1.vcf \\\n",
    "    ref_allele = ref from vcf, alt_allele = alt from vcf. \\n\"\n",
    "! echo \"{readme_text}\" >> {README}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66216d1-508d-49a6-b492-883f43aa6304",
   "metadata": {},
   "source": [
    "# Split files into chunks for downstream parallel analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266cfea-dac8-4636-b956-534911662895",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/Wheat/ZH_1K_exomes.txt\" #\"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/TAIR10_ZH_anc_vs_neutral.txt\"\n",
    "input_df = pd.read_csv(input_file, sep=\"\\t\", header=0, na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"]) # , compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aab8cb-bf02-42d6-8352-dce665db397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем размер чанка\n",
    "chunk_size = 100000\n",
    "\n",
    "for name_df, df in zip([input_file], [input_df]):\n",
    "    output_folder = name_df.replace(\".txt\", \"_chunks\")\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "        print(f\"The content of the {output_folder} was deleted\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    chunk_count = 0\n",
    "\n",
    "    # Добавляем сообщение перед началом цикла\n",
    "    print(f\"Splitting {len(df)} rows into chunks of size {chunk_size}\")\n",
    "    for i, chunk_start in enumerate(range(0, len(df), chunk_size)):\n",
    "        chunk = df.iloc[chunk_start:chunk_start + chunk_size]\n",
    "        chunk_file = os.path.join(output_folder, f\"chunk_{i+1}.tsv\")\n",
    "        chunk.to_csv(chunk_file, sep=\"\\t\", index=False)\n",
    "        chunk_count += 1\n",
    "        # Печать прогресса\n",
    "        if chunk_count % 10 == 0:\n",
    "            print(f\"{chunk_count} chunks saved...\")\n",
    "    print(f\"All chunks have been saved to the folder: {output_folder}. Total chunks = {chunk_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PlantCad2",
   "language": "python",
   "name": "plantcad2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
