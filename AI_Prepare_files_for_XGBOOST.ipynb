{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b3a00-a8a4-49fe-93f0-f338403e91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca6162-71f8-4130-a418-64540511a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parse pandas df. Extract columns, change start/end, add position, delete first and last 256 bp\n",
    "# drop_nan_strategy = any - will delete columns with NaN in any column. This is for Zero Shot\n",
    "# drop_nan_strategy = both - delete only if Nan in both; is for Fine Tuning\n",
    "def prepare_df_for_PlantCad_zeroshot(inp_df, ref_col, alt_col, frame=512, drop_nan_strategy=\"any\"):\n",
    "    df = inp_df.copy()\n",
    "    df = df[[\"chr\", \"start\", \"end\", ref_col, alt_col]]\n",
    "    \n",
    "    # Delete NaN \n",
    "    if drop_nan_strategy == \"any\":\n",
    "        df = df.dropna(subset=[ref_col, alt_col], how=\"any\")\n",
    "    elif drop_nan_strategy == \"both\":\n",
    "        df = df[~(df[ref_col].isna() & df[alt_col].isna())]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid drop_nan_strategy. Choose 'any' or 'both'.\")\n",
    "    \n",
    "    # Применяем фильтр для удаления строк, где 'start' меньше (frame/2 - 1)\n",
    "    df = df[df[\"start\"] >= (frame // 2 - 1)]\n",
    "    \n",
    "    # Создаем DataFrame df_zero_shot_input_coordinates. Координаты pos начинаются с 1\n",
    "    df_zero_shot_input_coordinates = pd.DataFrame({\n",
    "        \"chr\": df[\"chr\"],\n",
    "        \"start\": df[\"start\"] - (frame // 2 - 1),\n",
    "        \"end\": df[\"start\"] + 1 + (frame // 2),\n",
    "        \"pos\": df[\"start\"] + 1,\n",
    "        \"ref\": df[ref_col],\n",
    "        \"alt\": df[alt_col]\n",
    "    })\n",
    "    \n",
    "    return df_zero_shot_input_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a26a7-ef90-4ccb-ab77-4427c37a1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seqs_to_df_input_coordinates(genome_fasta, df_zero_shot_input_coordinates):\n",
    "    # Read the genome FASTA file\n",
    "    genome = {record.id: record.seq for record in SeqIO.parse(genome_fasta, \"fasta\")}\n",
    "    \n",
    "    # Define a function to extract the sequence based on coordinates\n",
    "    def extract_sequence(row):\n",
    "        chrom = row['chr']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        # Check if chromosome exists in genome\n",
    "        if chrom in genome:\n",
    "            # Check if 'end' is greater than the sequence length\n",
    "            seq_length = len(genome[chrom])\n",
    "            if end > seq_length:\n",
    "                return None\n",
    "            # Extract the subsequence, convert to string and uppercase\n",
    "            subsequence = genome[chrom][start:end] \n",
    "            return str(subsequence).upper()\n",
    "        else:\n",
    "            return None \n",
    "\n",
    "    # Add progress bar to the DataFrame processing\n",
    "    tqdm.pandas(desc=\"Adding sequences\")\n",
    "    df_zero_shot_input_coordinates['sequences'] = df_zero_shot_input_coordinates.progress_apply(extract_sequence, axis=1)\n",
    "    \n",
    "    # Drop rows where 'sequence' is None (i.e., where 'end' was out of bounds)\n",
    "    df_zero_shot_input_coordinates = df_zero_shot_input_coordinates.dropna(subset=['sequences'])\n",
    "    \n",
    "    return df_zero_shot_input_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dc86c-3468-4e77-948f-7d5a9a8229d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 255 to ref state\n",
    "def correct_sequence_ref(inp_df):\n",
    "    df = inp_df.copy()\n",
    "    \n",
    "    def modify_sequence(row):\n",
    "        # Заменяем символ на 255-й позиции в зависимости от label\n",
    "        char_to_replace = row[\"ref\"]\n",
    "        sequence = row[\"sequences\"]\n",
    "        modified_sequence = sequence[:255] + char_to_replace + sequence[256:]\n",
    "        return modified_sequence\n",
    "    tqdm.pandas(desc=\"Correcting sequences\")\n",
    "    df[\"sequences\"] = df.progress_apply(modify_sequence, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da7cc5-0735-4484-8cf6-a7f627dbd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column \"label\". ref = 0, alt = 1\n",
    "def add_label_column(df):\n",
    "    # Создаем DataFrame для строк, где только alt имеет значение\n",
    "    alt_only = df[pd.notna(df[\"alt\"]) & pd.isna(df[\"ref\"])].copy()\n",
    "    alt_only[\"label\"] = 1\n",
    "\n",
    "    # Создаем DataFrame для строк, где только ref имеет значение\n",
    "    ref_only = df[pd.notna(df[\"ref\"]) & pd.isna(df[\"alt\"])].copy()\n",
    "    ref_only[\"label\"] = 0\n",
    "\n",
    "    # Создаем DataFrame для строк, где оба значения не NaN\n",
    "    both = df[pd.notna(df[\"alt\"]) & pd.notna(df[\"ref\"])].copy()\n",
    "    both_1 = both.copy()\n",
    "    both_1[\"label\"] = 1\n",
    "    both_0 = both.copy()\n",
    "    both_0[\"label\"] = 0\n",
    "\n",
    "    # Объединяем все части в один DataFrame\n",
    "    result_df = pd.concat([alt_only, ref_only, both_1, both_0], ignore_index=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725ee8f-c019-437f-9317-85fafe6dd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sequence(df_fine_tune_input):\n",
    "    df = df_fine_tune_input.copy()\n",
    "    \n",
    "    def modify_sequence(row):\n",
    "        # Заменяем символ на 255-й позиции в зависимости от label\n",
    "        char_to_replace = row[\"alt\"] if row[\"label\"] == 1 else row[\"ref\"]\n",
    "        sequence = row[\"sequences\"]\n",
    "        modified_sequence = sequence[:255] + char_to_replace + sequence[256:]\n",
    "        return modified_sequence\n",
    "    # Применяем функцию с прогрессом к DataFrame\n",
    "    tqdm.pandas(desc=\"Correcting sequences\")\n",
    "    df[\"sequences\"] = df.progress_apply(modify_sequence, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37f71f-e82c-45ff-8af2-369e60231973",
   "metadata": {},
   "source": [
    "# Create files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4c315-6b06-494f-bdd3-f5d3d58bf0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for pop data (run on real data to find delterious mutations)\n",
    "allele_dataset = '/home/labs/alevy/omerbar/validations/AT/vcfs/AT_clean_vcf_chr_Chr1.bed.gz'\n",
    "genome_fasta = \"/home/labs/alevy/omerbar/backups/TAIR/A_thaliana.fa\"\n",
    "output_file = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/TAIR10_FT_pop_data_ref_alt.txt\"\n",
    "README = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/REAMDE.txt\"\n",
    "\n",
    "ref_col = 'ref_allele' #label = 0\n",
    "alt_col = 'alt_allele' #label = 1\n",
    "    \n",
    "# Use zero shot format to be changed next\n",
    "print('dataset loading')\n",
    "df_allele_dataset = pd.read_csv(allele_dataset, sep=\"\\t\", header=0, compression='gzip', na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"])\n",
    "df_allele_dataset = df_allele_dataset.rename(columns={'#chr': 'chr'})\n",
    "\n",
    "print('change coordinates')\n",
    "df_zero_shot_input_coordinates = prepare_df_for_PlantCad_zeroshot(df_allele_dataset, ref_col, alt_col, drop_nan_strategy=\"both\") \n",
    "print('adding seqs')\n",
    "df_zero_shot_input = add_seqs_to_df_input_coordinates(genome_fasta, df_zero_shot_input_coordinates)\n",
    "# change zero shot to fine tune format\n",
    "print('adding labels')\n",
    "df_fine_tune_input = add_label_column(df_zero_shot_input)\n",
    "print('correct sequences')\n",
    "df_fine_tune_input = correct_sequence(df_fine_tune_input)\n",
    "df_fine_tune_input = df_fine_tune_input.drop(columns=[\"start\", \"end\", \"ref\", \"alt\"])\n",
    "df_fine_tune_input.rename(columns={\"chr\": \"chrom\"}, inplace=True)\n",
    "print('inputing labels')\n",
    "df_fine_tune_input = df_fine_tune_input[[\"chrom\", \"pos\", \"label\", \"sequences\"]]\n",
    "print('saving file')\n",
    "df_fine_tune_input.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "\n",
    "readme_text = f\"{output_file}: ref_col = {ref_col}, alt_col = {alt_col}. ref_allele = label 0, alt_allele = label 1.\\n\"\n",
    "! echo \"{readme_text}\" >> {README}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c509f4-8a66-4537-a6da-1a1210d3f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcb31e-7768-40a9-b1d8-f9dd57005105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_txt = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/TAIR10_FT_neutral_vs_simulated.txt\"\n",
    "input_df= pd.read_csv(input_txt, sep=\"\\t\", header=0, na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"]) # , compression='gzip'\n",
    "test_df = input_df[input_df['chrom'] == 'Chr1']\n",
    "not_test_df = input_df[input_df['chrom'] != 'Chr1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98cba4-3ebc-4228-a522-0a98fe27479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split by chromosome ranges: split chromosome in partes (e.g. 10) and assign them to train and val randomly.\n",
    "# Это нужно чтобы уйти от рандомного перетасовывания - потому что могут быть пересекающиеся участки)\n",
    "\n",
    "# Разбиваем хромосому на фрагменты (в каждом может быть разные кол-во аллелей. Насколько это корректно?)\n",
    "n_segments = 100\n",
    "\n",
    "def assign_segments(group, n_segments):\n",
    "    min_pos = group['pos'].min()\n",
    "    max_pos = group['pos'].max()\n",
    "    segment_length = (max_pos - min_pos) / n_segments\n",
    "    \n",
    "    # Создаем границы интервалов\n",
    "    bins = [min_pos + i * segment_length for i in range(n_segments)]\n",
    "    bins.append(max_pos + 1)  # Добавляем правую границу\n",
    "    labels = list(range(1, n_segments + 1))  # Метки интервалов\n",
    "    \n",
    "    # Присваиваем отрезки\n",
    "    group['segment'] = pd.cut(group['pos'], bins=bins, labels=labels, include_lowest=True)\n",
    "    return group\n",
    "\n",
    "# Применяем функцию для каждой группы хромосом\n",
    "not_test_df_seg = not_test_df.groupby('chrom', group_keys=False).apply(assign_segments, n_segments=n_segments)\n",
    "\n",
    "# not_test_df_seg['segment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a2d38-834d-41a2-9013-e6d5b6abfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных: 80% на train, 20% на validation\n",
    "\n",
    "#train_df, val_df = train_test_split(not_test_df, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделяем данные на train и test в пределах каждой хромосомы\n",
    "def split_data_by_chrom_and_segment(df, test_size=0.2):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for chrom, group in df.groupby('chrom'):\n",
    "        # Уникальные сегменты для текущей хромосомы\n",
    "        segments = group[['chrom', 'segment']].drop_duplicates()\n",
    "        \n",
    "        # Разделение сегментов на train и test с разным random_state\n",
    "        train_segments, test_segments = train_test_split(\n",
    "            segments,\n",
    "            test_size=test_size,\n",
    "            random_state=np.random.randint(0, 10000)  # Случайное состояние для каждой хромосомы\n",
    "        )\n",
    "        \n",
    "        # Отбор строк для train и test\n",
    "        train_list.append(group.merge(train_segments, on=['chrom', 'segment']))\n",
    "        test_list.append(group.merge(test_segments, on=['chrom', 'segment']))\n",
    "    \n",
    "    # Объединяем результаты\n",
    "    train = pd.concat(train_list, ignore_index=True)\n",
    "    test = pd.concat(test_list, ignore_index=True)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Применяем функцию\n",
    "train_df, val_df = split_data_by_chrom_and_segment(not_test_df_seg)\n",
    "#segments_by_chrom = val_df.groupby('chrom')['segment'].unique()\n",
    "#segments_by_chrom - посмотреть как разбилось \n",
    "# Удалим колонку segment\n",
    "train_df = train_df.drop(columns=['segment'])\n",
    "val_df = val_df.drop(columns=['segment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f0083-9584-4e27-9cf6-ad2c52630951",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = input_txt.replace(\".txt\", \"_test.txt\")\n",
    "test_df.to_csv(output_test, index=False, sep=\"\\t\")\n",
    "\n",
    "output_train = input_txt.replace(\".txt\", \"_train.txt\")\n",
    "train_df.to_csv(output_train, index=False, sep=\"\\t\")\n",
    "\n",
    "output_val = input_txt.replace(\".txt\", \"_val.txt\")\n",
    "val_df.to_csv(output_val, index=False, sep=\"\\t\")\n",
    "\n",
    "README = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/REAMDE.txt\"\n",
    "\n",
    "readme_text = f\"*****split******\\n dataset {input_txt} was splitted in test (Chr1), train and val (Chr2345, 80/20) with use of 100 segments.\\n*******\"\n",
    "! echo \"{readme_text}\" >> {README}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846acc6-7686-47b4-81af-ad2a18de27bc",
   "metadata": {},
   "source": [
    "## Split by chunks for downstream parallel analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d050c94-17b8-41c0-a6ed-63711dad3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "input_txt = \"/home/labs/alevy/petrzhu/AI_workshop/PlantCaduceus/datasets/TAIR/TAIR10_FT_neutral_vs_simulated.txt\"\n",
    "\n",
    "output_test = input_txt.replace(\".txt\", \"_test.txt\")\n",
    "output_train = input_txt.replace(\".txt\", \"_train.txt\")\n",
    "output_val = input_txt.replace(\".txt\", \"_val.txt\")\n",
    "input_df= pd.read_csv(input_txt, sep=\"\\t\", header=0, na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"]) # , compression='gzip'\n",
    "test_df= pd.read_csv(output_test, sep=\"\\t\", header=0, na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"]) # , compression='gzip'\n",
    "train_df= pd.read_csv(output_train, sep=\"\\t\", header=0, na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"]) # , compression='gzip'\n",
    "val_df= pd.read_csv(output_val, sep=\"\\t\", header=0, na_values=[\"NA\", \"null\", \".\", \"-\", \"n/a\", \"N/A\", \"NaN\"]) # , compression='gzip'\n",
    "\n",
    "# Определяем размер чанка\n",
    "chunk_size = 100000\n",
    "\n",
    "# заменить output_train и train_df на нужные (например test or train or val...)\n",
    "for name_df, df in zip([output_val], [val_df]):\n",
    "    output_folder = name_df.replace(\".txt\", \"_chunks\")\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "        print(f\"The content of the {output_folder} was deleted\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    chunk_count = 0\n",
    "\n",
    "    # Добавляем сообщение перед началом цикла\n",
    "    print(f\"Splitting {len(df)} rows into chunks of size {chunk_size}\")\n",
    "    for i, chunk_start in enumerate(range(0, len(df), chunk_size)):\n",
    "        chunk = df.iloc[chunk_start:chunk_start + chunk_size]\n",
    "        chunk_file = os.path.join(output_folder, f\"chunk_{i+1}.tsv\")\n",
    "        chunk.to_csv(chunk_file, sep=\"\\t\", index=False)\n",
    "        chunk_count += 1\n",
    "        # Печать прогресса\n",
    "        if chunk_count % 10 == 0:\n",
    "            print(f\"{chunk_count} chunks saved...\")\n",
    "    print(f\"All chunks have been saved to the folder: {output_folder}. Total chunks = {chunk_count}\")\n",
    "\n",
    "\n",
    "# чет зависает, но работу делает"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PlantCad2",
   "language": "python",
   "name": "plantcad2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
